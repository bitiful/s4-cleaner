/*
 * Copyright (c) 2025 ç¼¤çº·äº‘S4 (Bitiful S4)
 *
 * ç¼¤çº·äº‘S3ä¸´æ—¶æ–‡ä»¶æ¸…ç†å·¥å…·
 * Bitiful S4 S3 Temporary File Cleaner
 */

package cleaner

import (
	"context"
	"encoding/csv"
	"encoding/json"
	"fmt"
	"os"
	"strings"
	"time"
	"unicode/utf8"

	"github.com/aws/aws-sdk-go-v2/aws"
	awsconfig "github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/credentials"
	"github.com/aws/aws-sdk-go-v2/service/s3"
	"github.com/bitiful/s4-cleaner/pkg/config"
	"github.com/fatih/color"
	"github.com/olekukonko/tablewriter"
)

// S3Cleaner S3æ¸…ç†å™¨
// S3Cleaner is a cleaner for S3 buckets
type S3Cleaner struct {
	client *s3.Client
	cfg    *config.Config
}

// FileInfo æ–‡ä»¶ä¿¡æ¯
// FileInfo contains information about a file
type FileInfo struct {
	Bucket        string    `json:"bucket"`
	Key           string    `json:"key"`
	Size          int64     `json:"size"`
	ModTime       time.Time `json:"mod_time"`
	ShouldDelete  bool      `json:"should_delete"`
	DeleteSuccess *bool     `json:"delete_success,omitempty"`
}

// NewS3Cleaner åˆ›å»ºæ–°çš„S3æ¸…ç†å™¨
// NewS3Cleaner creates a new S3 cleaner
func NewS3Cleaner(accessKey, secretKey string, cfg *config.Config) (*S3Cleaner, error) {
	// è§£æè¿‡æœŸæ—¶é—´
	// Parse expiration time
	if err := cfg.ParseTime(); err != nil {
		return nil, err
	}

	// åˆ›å»ºAWSé…ç½®
	// Create AWS configuration
	awsCfg, err := awsconfig.LoadDefaultConfig(context.TODO(),
		awsconfig.WithCredentialsProvider(credentials.NewStaticCredentialsProvider(accessKey, secretKey, "")),
		awsconfig.WithRegion("us-east-1"), // é»˜è®¤åŒºåŸŸï¼Œä¼šæ ¹æ®æ¡¶è‡ªåŠ¨è°ƒæ•´ | Default region, will be adjusted automatically based on bucket
	)
	if err != nil {
		return nil, fmt.Errorf("æ— æ³•åŠ è½½AWSé…ç½®: %v\nFailed to load AWS configuration: %v", err, err)
	}

	// åˆ›å»ºS3å®¢æˆ·ç«¯
	// Create S3 client
	client := s3.NewFromConfig(awsCfg)

	return &S3Cleaner{
		client: client,
		cfg:    cfg,
	}, nil
}

// Run æ‰§è¡Œæ¸…ç†æ“ä½œ
// Run executes the cleaning operation
func (c *S3Cleaner) Run() error {
	var buckets []string
	var err error

	// å¦‚æœæŒ‡å®šäº†æ¡¶ï¼Œåˆ™åªå¤„ç†è¯¥æ¡¶
	// If bucket is specified, only process that bucket
	if c.cfg.Bucket != "" {
		buckets = []string{c.cfg.Bucket}
	} else {
		// å¦åˆ™è·å–æ‰€æœ‰æ¡¶
		// Otherwise get all buckets
		buckets, err = c.listBuckets()
		if err != nil {
			return err
		}
	}

	// æ”¶é›†æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯
	// Collect all file information
	allFiles := []FileInfo{}

	// å¤„ç†æ¯ä¸ªæ¡¶
	// Process each bucket
	for _, bucket := range buckets {
		files, err := c.processOneBucket(bucket)
		if err != nil {
			color.Red("å¤„ç†æ¡¶ %s æ—¶å‡ºé”™: %v\nError processing bucket %s: %v", bucket, err, bucket, err)
			continue
		}
		allFiles = append(allFiles, files...)
	}

	// æ ¹æ®æ ¼å¼è¾“å‡ºç»“æœ
	// Output results based on format
	switch strings.ToLower(c.cfg.Format) {
	case "json":
		return c.outputJSON(allFiles)
	case "csv":
		return c.outputCSV(allFiles)
	default: // table
		return c.outputTable(allFiles)
	}
}

// listBuckets åˆ—å‡ºæ‰€æœ‰æ¡¶
// listBuckets lists all buckets
func (c *S3Cleaner) listBuckets() ([]string, error) {
	resp, err := c.client.ListBuckets(context.TODO(), &s3.ListBucketsInput{})
	if err != nil {
		return nil, fmt.Errorf("æ— æ³•åˆ—å‡ºæ¡¶: %v\nFailed to list buckets: %v", err, err)
	}

	buckets := make([]string, 0, len(resp.Buckets))
	for _, bucket := range resp.Buckets {
		buckets = append(buckets, *bucket.Name)
	}

	return buckets, nil
}

// processOneBucket å¤„ç†ä¸€ä¸ªæ¡¶
// processOneBucket processes one bucket
func (c *S3Cleaner) processOneBucket(bucket string) ([]FileInfo, error) {
	// color.Cyan("æ­£åœ¨å¤„ç†æ¡¶: %s\nProcessing bucket: %s", bucket, bucket)

	var keyMarker *string
	var uploadIdMarker *string
	files := []FileInfo{}

	// åˆ†é¡µåˆ—å‡ºæ‰€æœ‰æœªå®Œæˆçš„åˆ†æ®µä¸Šä¼ 
	// List all multipart uploads with pagination
	for {
		resp, err := c.client.ListMultipartUploads(context.TODO(), &s3.ListMultipartUploadsInput{
			Bucket:         aws.String(bucket),
			KeyMarker:      keyMarker,
			UploadIdMarker: uploadIdMarker,
		})
		if err != nil {
			return nil, fmt.Errorf("æ— æ³•åˆ—å‡ºæ¡¶ %s ä¸­çš„æœªå®Œæˆä¸Šä¼ : %v\nFailed to list multipart uploads in bucket %s: %v", bucket, err, bucket, err)
		}

		// å¤„ç†å½“å‰é¡µçš„æœªå®Œæˆä¸Šä¼ 
		// Process uploads in current page
		for _, upload := range resp.Uploads {
			// æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
			// Check if it's expired
			shouldDelete := upload.Initiated.Before(c.cfg.ExpirationTime)

			// è·å–å¯¹è±¡å¤§å°
			// Get object size
			var size int64
			parts, err := c.client.ListParts(context.TODO(), &s3.ListPartsInput{
				Bucket:   aws.String(bucket),
				Key:      upload.Key,
				UploadId: upload.UploadId,
			})
			if err == nil {
				for _, part := range parts.Parts {
					if part.Size != nil {
						size += *part.Size
					}
				}
			}

			fileInfo := FileInfo{
				Bucket:       bucket,
				Key:          *upload.Key,
				Size:         size,
				ModTime:      *upload.Initiated,
				ShouldDelete: shouldDelete,
			}

			// å¦‚æœéœ€è¦åˆ é™¤ä¸”doæ ‡å¿—ä¸ºtrueï¼Œåˆ™æ‰§è¡Œåˆ é™¤
			// If should delete and do flag is true, perform deletion
			if shouldDelete && c.cfg.DoDelete {
				success := c.abortMultipartUpload(bucket, *upload.Key, *upload.UploadId)
				fileInfo.DeleteSuccess = &success
			}

			files = append(files, fileInfo)
		}

		// å¦‚æœæ²¡æœ‰æ›´å¤šé¡µï¼Œåˆ™é€€å‡ºå¾ªç¯
		// If no more pages, exit loop
		if resp.IsTruncated == nil || !*resp.IsTruncated {
			break
		}
		keyMarker = resp.NextKeyMarker
		uploadIdMarker = resp.NextUploadIdMarker
	}

	return files, nil
}

// abortMultipartUpload ä¸­æ­¢åˆ†æ®µä¸Šä¼ 
// abortMultipartUpload aborts a multipart upload
func (c *S3Cleaner) abortMultipartUpload(bucket, key, uploadId string) bool {
	_, err := c.client.AbortMultipartUpload(context.TODO(), &s3.AbortMultipartUploadInput{
		Bucket:   aws.String(bucket),
		Key:      aws.String(key),
		UploadId: aws.String(uploadId),
	})
	return err == nil
}

// truncateString æˆªæ–­å­—ç¬¦ä¸²ï¼Œç¡®ä¿ä¸­æ—¥éŸ©æ–‡å­—ç¬¦å ä¸¤ä¸ªå­—èŠ‚
// truncateString truncates string, ensuring CJK characters count as two bytes
func truncateString(s string, maxBytes int) string {
	// è®¡ç®—å­—ç¬¦ä¸²çš„å­—èŠ‚å®½åº¦
	// Calculate string byte width
	var totalWidth int
	for _, r := range s {
		var runeWidth int
		if r >= 0x4E00 && r <= 0x9FFF || // CJKç»Ÿä¸€è¡¨æ„æ–‡å­— | CJK Unified Ideographs
			r >= 0x3040 && r <= 0x309F || // å¹³å‡å | Hiragana
			r >= 0x30A0 && r <= 0x30FF || // ç‰‡å‡å | Katakana
			r >= 0x3400 && r <= 0x4DBF || // CJKç»Ÿä¸€è¡¨æ„æ–‡å­—æ‰©å±•A | CJK Unified Ideographs Extension A
			r >= 0x20000 && r <= 0x2A6DF || // CJKç»Ÿä¸€è¡¨æ„æ–‡å­—æ‰©å±•B | CJK Unified Ideographs Extension B
			r >= 0x2A700 && r <= 0x2B73F || // CJKç»Ÿä¸€è¡¨æ„æ–‡å­—æ‰©å±•C | CJK Unified Ideographs Extension C
			r >= 0x2B740 && r <= 0x2B81F || // CJKç»Ÿä¸€è¡¨æ„æ–‡å­—æ‰©å±•D | CJK Unified Ideographs Extension D
			r >= 0x2B820 && r <= 0x2CEAF || // CJKç»Ÿä¸€è¡¨æ„æ–‡å­—æ‰©å±•E | CJK Unified Ideographs Extension E
			r >= 0xAC00 && r <= 0xD7AF || // æœé²œæ–‡éŸ³èŠ‚ | Hangul Syllables
			r >= 0xF900 && r <= 0xFAFF || // CJKå…¼å®¹è¡¨æ„æ–‡å­— | CJK Compatibility Ideographs
			r >= 0xFF00 && r <= 0xFFEF {
			runeWidth = 2
		} else {
			runeWidth = 1
		}
		totalWidth += runeWidth
	}

	// å¦‚æœæ€»å®½åº¦ä¸è¶…è¿‡æœ€å¤§å®½åº¦ï¼Œåˆ™ä¸éœ€è¦æˆªæ–­
	// If total width does not exceed max width, no need to truncate
	if totalWidth <= maxBytes {
		return s
	}

	// éœ€è¦æˆªæ–­
	// Need to truncate
	var currentBytes int
	var truncated []rune
	for _, r := range s {
		var runeWidth int
		if r >= 0x4E00 && r <= 0x9FFF || // CJKç»Ÿä¸€è¡¨æ„æ–‡å­— | CJK Unified Ideographs
			r >= 0x3040 && r <= 0x309F || // å¹³å‡å | Hiragana
			r >= 0x30A0 && r <= 0x30FF || // ç‰‡å‡å | Katakana
			r >= 0x3400 && r <= 0x4DBF || // CJKç»Ÿä¸€è¡¨æ„æ–‡å­—æ‰©å±•A | CJK Unified Ideographs Extension A
			r >= 0x20000 && r <= 0x2A6DF || // CJKç»Ÿä¸€è¡¨æ„æ–‡å­—æ‰©å±•B | CJK Unified Ideographs Extension B
			r >= 0x2A700 && r <= 0x2B73F || // CJKç»Ÿä¸€è¡¨æ„æ–‡å­—æ‰©å±•C | CJK Unified Ideographs Extension C
			r >= 0x2B740 && r <= 0x2B81F || // CJKç»Ÿä¸€è¡¨æ„æ–‡å­—æ‰©å±•D | CJK Unified Ideographs Extension D
			r >= 0x2B820 && r <= 0x2CEAF || // CJKç»Ÿä¸€è¡¨æ„æ–‡å­—æ‰©å±•E | CJK Unified Ideographs Extension E
			r >= 0xAC00 && r <= 0xD7AF || // æœé²œæ–‡éŸ³èŠ‚ | Hangul Syllables
			r >= 0xF900 && r <= 0xFAFF || // CJKå…¼å®¹è¡¨æ„æ–‡å­— | CJK Compatibility Ideographs
			r >= 0xFF00 && r <= 0xFFEF {
			runeWidth = 2
		} else {
			runeWidth = 1
		}

		if currentBytes+runeWidth > maxBytes-3 { // ä¸ºçœç•¥å·é¢„ç•™ç©ºé—´ | Reserve space for ellipsis
			break
		}

		truncated = append(truncated, r)
		currentBytes += runeWidth
	}

	return string(truncated) + "..."
}

// outputTable ä»¥è¡¨æ ¼å½¢å¼è¾“å‡ºç»“æœ
// outputTable outputs results in table format
func (c *S3Cleaner) outputTable(files []FileInfo) error {
	table := tablewriter.NewWriter(os.Stdout)
	table.SetHeader([]string{"å­˜å‚¨æ¡¶ | Bucket", "é”® | Key", "å¤§å° | Size", "ä¿®æ”¹æ—¶é—´ | Mod Time", "çŠ¶æ€ | Status"})
	table.SetAutoWrapText(false)
	table.SetAutoFormatHeaders(true)
	table.SetHeaderColor(
		tablewriter.Colors{tablewriter.Bold, tablewriter.FgCyanColor},
		tablewriter.Colors{tablewriter.Bold, tablewriter.FgCyanColor},
		tablewriter.Colors{tablewriter.Bold, tablewriter.FgCyanColor},
		tablewriter.Colors{tablewriter.Bold, tablewriter.FgCyanColor},
		tablewriter.Colors{tablewriter.Bold, tablewriter.FgCyanColor},
	)

	// è®¾ç½®åˆ—å®½ï¼Œå¢åŠ  Key åˆ—çš„å®½åº¦
	// Set column width, increase Key column width
	table.SetColWidth(120)

	for _, file := range files {
		// æˆªæ–­è¿‡é•¿çš„é”®
		// Truncate long keys
		key := file.Key
		if utf8.RuneCountInString(key)*3 > 120 {
			key = truncateString(key, 120)
		}

		// æ ¼å¼åŒ–æ—¶é—´
		// Format time
		timeStr := file.ModTime.Format("2006-01-02 15:04:05")

		// æ ¼å¼åŒ–å¤§å°
		// Format size
		sizeStr := formatSize(file.Size)

		// æ ¼å¼åŒ–çŠ¶æ€ï¼Œä½¿ç”¨è¡¨æƒ…ç¬¦å·å’Œæ–‡å­—
		// Format status with emoji and text
		var statusStr string
		var statusColor tablewriter.Colors

		if file.DeleteSuccess == nil {
			// æœªæ‰§è¡Œåˆ é™¤æ“ä½œæ—¶ï¼Œæ˜¾ç¤ºæ˜¯å¦ä¼šè¢«å‘½ä¸­åˆ é™¤
			// When deletion is not executed, show if it would be targeted for deletion
			if file.ShouldDelete {
				statusStr = "ğŸ¯ Will delete" // ä¼šè¢«å‘½ä¸­åˆ é™¤ | Would be targeted for deletion
				statusColor = tablewriter.Colors{tablewriter.FgHiRedColor}
			} else {
				statusStr = "ğŸ” Won't delete" // ä¸ä¼šè¢«å‘½ä¸­åˆ é™¤ | Would not be targeted for deletion
				statusColor = tablewriter.Colors{tablewriter.FgYellowColor}
			}
		} else if *file.DeleteSuccess {
			statusStr = "âœ… Deleted" // åˆ é™¤æˆåŠŸ | Deletion Success
			statusColor = tablewriter.Colors{tablewriter.FgGreenColor}
		} else {
			statusStr = "âŒ Delete failed" // åˆ é™¤å¤±è´¥ | Deletion Failed
			statusColor = tablewriter.Colors{tablewriter.FgRedColor}
		}

		// æ ¹æ®æ˜¯å¦åº”åˆ é™¤è®¾ç½®æ—¶é—´åˆ—çš„é¢œè‰²
		// Set time column color based on should delete
		var timeColor tablewriter.Colors
		if file.ShouldDelete {
			timeColor = tablewriter.Colors{tablewriter.FgHiRedColor}
		} else {
			timeColor = tablewriter.Colors{tablewriter.FgYellowColor}
		}

		row := []string{file.Bucket, key, sizeStr, timeStr, statusStr}
		colors := []tablewriter.Colors{
			tablewriter.Colors{tablewriter.FgHiBlueColor},
			tablewriter.Colors{tablewriter.FgWhiteColor},
			tablewriter.Colors{tablewriter.FgHiCyanColor},
			timeColor,
			statusColor,
		}

		table.Rich(row, colors)
	}

	if len(files) == 0 {
		color.Yellow("æœªæ‰¾åˆ°ä¸´æ—¶æ–‡ä»¶\nNo temporary files found")
	} else {
		table.Render()

		// è®¡ç®—æ€»å®¹é‡å’Œåº”åˆ é™¤çš„å®¹é‡
		// Calculate total size and size to delete
		var totalSize, sizeToDelete, sizeDeleted int64
		var countToDelete, countDeleted int
		for _, file := range files {
			totalSize += file.Size
			if file.ShouldDelete {
				sizeToDelete += file.Size
				countToDelete++
			}
			// è®¡ç®—å·²åˆ é™¤çš„æ–‡ä»¶æ•°é‡å’Œå¤§å°
			// Calculate deleted files count and size
			if file.DeleteSuccess != nil && *file.DeleteSuccess {
				sizeDeleted += file.Size
				countDeleted++
			}
		}

		// è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
		// Output statistics
		fmt.Println()

		// åˆ›å»ºç»Ÿè®¡ä¿¡æ¯è¡¨æ ¼
		// Create statistics table
		statTable := tablewriter.NewWriter(os.Stdout)
		statTable.SetHeader([]string{"ç»Ÿè®¡ä¿¡æ¯ | Statistics", "å€¼ | Value"})
		statTable.SetAutoWrapText(false)
		statTable.SetAutoFormatHeaders(true)
		statTable.SetHeaderColor(
			tablewriter.Colors{tablewriter.Bold, tablewriter.FgCyanColor},
			tablewriter.Colors{tablewriter.Bold, tablewriter.FgCyanColor},
		)
		statTable.SetColumnColor(
			tablewriter.Colors{tablewriter.FgHiWhiteColor},
			tablewriter.Colors{tablewriter.FgHiWhiteColor},
		)

		// æ·»åŠ ç»Ÿè®¡æ•°æ®è¡Œ
		// Add statistics data rows
		statTable.Append([]string{"æ€»æ–‡ä»¶æ•° | Total files", fmt.Sprintf("%d", len(files))})
		statTable.Append([]string{"æ€»å®¹é‡ | Total size", formatSize(totalSize)})
		statTable.Append([]string{"åº”åˆ é™¤æ–‡ä»¶æ•° | Files to delete", fmt.Sprintf("%d", countToDelete)})
		statTable.Append([]string{"åº”åˆ é™¤å®¹é‡ | Size to delete", formatSize(sizeToDelete)})
		statTable.Append([]string{"å·²åˆ é™¤æ–‡ä»¶æ•° | Files deleted", fmt.Sprintf("%d", countDeleted)})
		statTable.Append([]string{"å·²åˆ é™¤å®¹é‡ | Size deleted", formatSize(sizeDeleted)})

		// æ¸²æŸ“ç»Ÿè®¡è¡¨æ ¼
		// Render statistics table
		statTable.Render()
	}
	return nil
}

// formatSize æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
// formatSize formats file size
func formatSize(size int64) string {
	const (
		B  = 1
		KB = 1024 * B
		MB = 1024 * KB
		GB = 1024 * MB
		TB = 1024 * GB
	)

	switch {
	case size >= TB:
		return fmt.Sprintf("%.2f TB", float64(size)/float64(TB))
	case size >= GB:
		return fmt.Sprintf("%.2f GB", float64(size)/float64(GB))
	case size >= MB:
		return fmt.Sprintf("%.2f MB", float64(size)/float64(MB))
	case size >= KB:
		return fmt.Sprintf("%.2f KB", float64(size)/float64(KB))
	default:
		return fmt.Sprintf("%d B", size)
	}
}

// outputJSON ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœ
// outputJSON outputs results in JSON format
func (c *S3Cleaner) outputJSON(files []FileInfo) error {
	result := struct {
		Files []FileInfo `json:"files"`
		Total int        `json:"total"`
	}{
		Files: files,
		Total: len(files),
	}

	jsonData, err := json.MarshalIndent(result, "", "  ")
	if err != nil {
		return fmt.Errorf("æ— æ³•åºåˆ—åŒ–ä¸ºJSON: %v\nFailed to serialize to JSON: %v", err, err)
	}

	fmt.Println(string(jsonData))
	return nil
}

// outputCSV ä»¥CSVæ ¼å¼è¾“å‡ºç»“æœ
// outputCSV outputs results in CSV format
func (c *S3Cleaner) outputCSV(files []FileInfo) error {
	writer := csv.NewWriter(os.Stdout)
	defer writer.Flush()

	// å†™å…¥è¡¨å¤´
	// Write header
	if err := writer.Write([]string{"Bucket", "Key", "Size", "ModTime", "ShouldDelete", "DeleteSuccess"}); err != nil {
		return fmt.Errorf("æ— æ³•å†™å…¥CSVè¡¨å¤´: %v\nFailed to write CSV header: %v", err, err)
	}

	// å†™å…¥æ•°æ®
	// Write data
	for _, file := range files {
		shouldDelete := "false"
		if file.ShouldDelete {
			shouldDelete = "true"
		}

		deleteSuccess := "not_executed"
		if file.DeleteSuccess != nil {
			if *file.DeleteSuccess {
				deleteSuccess = "true"
			} else {
				deleteSuccess = "false"
			}
		}

		if err := writer.Write([]string{
			file.Bucket,
			file.Key,
			fmt.Sprintf("%d", file.Size),
			file.ModTime.Format(time.RFC3339),
			shouldDelete,
			deleteSuccess,
		}); err != nil {
			return fmt.Errorf("æ— æ³•å†™å…¥CSVæ•°æ®: %v\nFailed to write CSV data: %v", err, err)
		}
	}

	return nil
}
